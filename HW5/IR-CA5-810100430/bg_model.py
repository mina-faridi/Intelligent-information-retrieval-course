# -*- coding: utf-8 -*-
"""part2-bc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfuzb1S3t0Yc8dn1lLE-c3qcJL-CpUQw
"""

from numpy import zeros, int8, log
from pylab import random
import sys
import jieba
import re
import time
import codecs
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

# WARNING: YOU DO NOT NEED TO CHANGE PREPROCESSING FUNCTION!
# segmentation, stopwords filtering and document-word matrix generating
# [return]:
# N : number of documents
# M : length of dictionary
# word2id : a map mapping terms to their corresponding ids
# id2word : a map mapping ids to terms
# X : document-word matrix, N*M, each line is the number of terms that show up in the document
def preprocessing(datasetFilePath):

      #stopwords
    stops = set(stopwords.words("english"))
    # read the documents
    file = codecs.open(datasetFilePath, 'r', 'utf-8')
    documents = [document.strip() for document in file]
    file.close()

    # number of documents
    N = len(documents)

    wordCounts = [];
    word2id = {}
    id2word = {}
    currentId = 0;
    # generate the word2id and id2word maps and count the number of times of words showing up in documents
    for document in documents:
        segList = jieba.cut(document)
        wordCount = {}
        for word in segList:
            word = word.lower().strip()
            if len(word) > 1 and not re.search('[0-9]', word) and word not in stops:
                if word not in word2id.keys():
                    word2id[word] = currentId;
                    id2word[currentId] = word;
                    currentId += 1;
                if word in wordCount:
                    wordCount[word] += 1
                else:
                    wordCount[word] = 1
        wordCounts.append(wordCount);

    # length of dictionary
    M = len(word2id)

    # generate the document-word matrix
    X = zeros([N, M], int8)
    for word in word2id.keys():
        j = word2id[word]
        for i in range(0, N):
            if word in wordCounts[i]:
                X[i, j] = wordCounts[i][word];
    

    


    return N, M, word2id, id2word, X


def initializeParameters():
    for i in range(0, N):
        normalization = sum(theta[i, :])
        for j in range(0, K):
            theta[i, j] /= normalization;

    for i in range(0, K):
        normalization = sum(beta[i, :])
        for j in range(0, M):
            beta[i, j] /= normalization;


def EStep():
    for i in range(0, N):
        for j in range(0, M):
            sum = 0
            for k in range(0, K):
                p[i, j, k] = beta[k, j] * theta[i, k]
                sum += beta[k, j] * theta[i, k]
            for k in range(0, K):
                p[i, j, k] /= float(sum)

def MStep():
    for k in range(0, K):
        sum = 0
        for j in range(0, M):
            for i in range(0, N):
                beta[k, j] += p[i, j, k] * X[i, j]
                sum += p[i, j, k] * X[i, j]
        for j in range(0, M):
            beta[k, j] /= float(sum)
    for i in range(0, N):
        sum = 0
        for k in range(0, K):
            for j in range(0, M):
                theta[i, k] += p[i, j, k] * X[i, j]
                sum += p[i, j, k] * X[i, j]
        for k in range(0, K):
            theta[i, k] /= float(sum)




# calculate the log likelihood
def LogLikelihood():
    loglikelihood = 0
    for i in range(0, N):
        loglikelihood += log(1/float(N))
        for j in range(0, M):
            p = 0
            for k in range(0, K):
                p += theta[i, k] * beta[k, j]
            loglikelihood += log(p) * X[i, j]
    return loglikelihood


# calculate the log likelihood with Background
def LogLikelihoodBG(lamb):
    
    s = sum(X)
    Wordall = sum(s)
    
    BGround = zeros([M], int8)

    for i in range(0,M):
      wordDocs = sum(X.T[i])

      BGround[i]=wordDocs/Wordall

    loglikelihood = 0
    for i in range(0, N):
        loglikelihood += log(1/float(N))
        for j in range(0, M):
            p = BG[j]*lamb
            for k in range(0, K):
                p += (theta[i, k] * beta[k, j])*(1-lamb)
            loglikelihood += log(p) * X[i, j]
    return loglikelihood


# output the params of model and top words of topics to files
def output():
    # document-topic distribution
    file = codecs.open(docTopicDist,'w','utf-8')
    for i in range(0, N):
        tmp = ''
        for j in range(0, K):
            tmp += str(theta[i, j]) + ' '
        file.write(tmp + '\n')
    file.close()

    # topic-word distribution
    file = codecs.open(topicWordDist,'w','utf-8')
    for i in range(0, K):
        tmp = ''
        for j in range(0, M):
            tmp += str(beta[i, j]) + ' '
        file.write(tmp + '\n')
    file.close()

    # dictionary
    file = codecs.open(dictionary,'w','utf-8')
    for i in range(0, M):
        file.write(id2word[i] + '\n')
    file.close()

    # top words of each topic
    file = codecs.open(topicWords,'w','utf-8')
    for i in range(0, K):
        topicword = []
        ids = beta[i, :].argsort()
        for j in ids:
            topicword.insert(0, id2word[j])
        tmp = ''
        for word in topicword[0:min(topicWordsNum, len(topicword))]:
            tmp += word + ' '
        file.write(tmp + '\n')
    file.close()



#### Starting main program ####
datasetFilePath = "/content/drive/My Drive/dataset.txt" # or 'dataset2.txt'
# stopwordsFilePath = 'stopwords.dic'
K = 10   # number of topic
maxIteration = 20
threshold = 5
topicWordsNum = 10
docTopicDist = '/content/drive/My Drive/docTopicDistribution.txt'
topicWordDist = '/content/drive/My Drive/topicWordDistribution.txt'
dictionary = '/content/drive/My Drive/dictionary.dic'
topicWords = '/content/drive/My Drive/topics.txt'

# preprocessing
N, M, word2id, id2word, X = preprocessing(datasetFilePath)

# theta[i, j] : p(zj|di): 2-D matrix
theta = random([N, K])
# beta[i, j] : p(wj|zi): 2-D matrix
beta = random([K, M])
# p[i, j, k] : p(zk|di,wj): 3-D tensor
p = zeros([N, M, K])

initializeParameters() # normarlize

# EM algorithm
oldLoglikelihood = 1
newLoglikelihood = 1
for i in range(0, maxIteration):
    EStep() 
    MStep()


    # newLoglikelihood = LogLikelihood()
    newLoglikelihood = LogLikelihoodBG(0.9)
    if(oldLoglikelihood != 1 and newLoglikelihood - oldLoglikelihood < threshold):
        break
    oldLoglikelihood = newLoglikelihood

output()

